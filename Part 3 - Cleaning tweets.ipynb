{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db15225",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_tweets = pd.read_csv('pres_tweets.csv', index_col=0)\n",
    "pres_tweets.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "pres_tweets = pres_tweets.reset_index(drop=True)\n",
    "pres_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f01e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(x):\n",
    "    if type(x) != str:\n",
    "        return ''\n",
    "    else:\n",
    "        x = re.sub('@[\\w]*', ' ', x)\n",
    "        x = re.sub('http\\S+',' ',x)\n",
    "        x = re.sub('www\\S+',' ',x)\n",
    "        x = x.lower()\n",
    "        x = re.findall('[A-za-z]+', x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = list(map(clean_tweets, pres_tweets['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bfd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of candidate names\n",
    "c_names = ['Joe Biden',\n",
    " 'Howie Hawkins',\n",
    " 'Donald Trump',\n",
    " 'Jo Jorgensen',\n",
    " 'Gary Johnson',\n",
    " 'Darrell Castle',\n",
    " 'Evan McMullin',\n",
    " 'Jill Stein',\n",
    " 'Donald Trump',\n",
    " 'Hillary Clinton',\n",
    " 'Mitt Romney',\n",
    " 'Gary Johnson',\n",
    " 'Jill Stein',\n",
    " 'Barack Obama',\n",
    " 'John McCain',\n",
    " 'Ralph Nader',\n",
    " 'Bob Barr',\n",
    " 'Barack Obama',\n",
    " 'Chuck Baldwin',\n",
    " 'Cynthia McKinney']\n",
    "c_names = [x.lower() for name in c_names for x in name.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de7ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find most used words for each candidate\n",
    "def make_bag(candidate):\n",
    "    cand_df = pres_tweets[pres_tweets['candidate']== candidate]\n",
    "    index = cand_df.index\n",
    "    word_bag = ''\n",
    "    for tweet in cleaned_tweets[index[0]:index[-1]+1]:\n",
    "        for word in tweet:\n",
    "            if word not in c_names:\n",
    "                word_bag = word_bag+' '+word\n",
    "    return word_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_bag = make_bag('Joe Biden')\n",
    "len(biden_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"biden.txt\", \"w\")\n",
    "text.write(biden_bag)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_bag = make_bag('Donald Trump')\n",
    "text = open(\"trump.txt\", \"w\")\n",
    "text.write(trump_bag)\n",
    "text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f479afb",
   "metadata": {},
   "source": [
    "# lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3acfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c8a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(l):\n",
    "    \n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper() \n",
    "        tag_dict = {\"J\": wordnet.ADJ, \n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lemmatized = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in l]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time lem for 100 tweets:\n",
    "from time import perf_counter\n",
    "\n",
    "temp = cleaned_tweets[0:100]\n",
    "start = perf_counter()\n",
    "t = list(map(lemmatize, temp))\n",
    "end = perf_counter()\n",
    "execution_time = (end - start)\n",
    "print(execution_time) #.55 seconds for 100 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_pres_tweets = list(map(lemmatize, cleaned_tweets))\n",
    "#started 12:15-12:26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    return ' '.join([word for word in l if not word in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51159bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test 100 tweets for time\n",
    "start = perf_counter()\n",
    "a = list(map(remove_stopwords, t))\n",
    "end = perf_counter()\n",
    "execution_time = (end - start)\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords2(l):\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    return ' '.join([word for word in l if word not in cachedStopWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7256c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test 100 tweets for time\n",
    "start = perf_counter()\n",
    "a = list(map(remove_stopwords2, t))\n",
    "end = perf_counter()\n",
    "execution_time = (end - start)\n",
    "print(execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe08bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_tweets['tweet_processed'] = list(map(remove_stopwords2, lemmatized_pres_tweets))\n",
    "#started 1:42-45?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66562010",
   "metadata": {},
   "source": [
    "# gubernatorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de18e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gub_tweets = pd.read_csv('gub_tweets.csv', index_col=0)\n",
    "gub_tweets.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "gub_tweets = gub_tweets.reset_index(drop=True)\n",
    "gub_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove tweets that might not be about the candidate\n",
    "gub = pd.read_csv('gub.csv', index_col=0)\n",
    "#make dictionary of candidates with state\n",
    "state_dict = {gub['candidate'][i].strip(): gub['state'][i] for i in range(len(gub))}\n",
    "state_dict['Pat Quinn'] = 'Illinois'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d48226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets2(x):\n",
    "    if type(x) != str:\n",
    "        return np.nan\n",
    "    elif key_words[0] not in x or key_words[1] not in x or key_words[2] not in x:\n",
    "        return np.nan\n",
    "    else: \n",
    "        x = re.sub('@[\\w]*', ' ', x)\n",
    "        x = re.sub('http\\S+',' ',x)\n",
    "        x = x.lower()\n",
    "        x = re.findall('[A-za-z]+', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d664ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop null tweets from df\n",
    "#gub_tweets.dropna(subset = [\"tweet\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc5ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets_gub = list(map(clean_tweets, gub_tweets['tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7eb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tweets = list(map(lemmatize, cleaned_tweets_gub))\n",
    "#started 12:35-1:17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80010cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gub_tweets['tweet_processed'] = list(map(remove_stopwords2, lemmatized_tweets))\n",
    "#started 1:34-42?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gub_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d267fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_tweets.to_csv('pres_tweets_processed.csv')\n",
    "gub_tweets.to_csv('gub_tweets_processed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
